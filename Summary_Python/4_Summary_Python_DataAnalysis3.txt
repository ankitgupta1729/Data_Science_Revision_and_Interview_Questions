(1) Reading and Writing Data:

!type ex1.csv

--

a,b,c,d,message
1,2,3,4,hello
5,6,7,8,world
9,10,11,12,foo

pd.read_csv('ex1.csv')
pd.read_table('ex1.csv', sep=',')


(2)

import pandas as pd
fold='.\\pydata-book-3rd-edition\\examples'
print(pd.read_csv(fold+'\\ex2.csv',names=['a','b','c','d','message']))

--

   a   b   c   d message
0  1   2   3   4   hello
1  5   6   7   8   world
2  9  10  11  12     foo

(3)

Suppose you wanted the message column to be the index of the returned DataFrame. 
You can either indicate you want the column at index 4 or named 'message' using the index_col argument:

print(pd.read_csv(fold+'\\ex2.csv',names=['a','b','c','d','message'],index_col='message'))

--

         a   b   c   d
message               
hello    1   2   3   4
world    5   6   7   8
foo      9  10  11  12

(4)

print(pd.read_csv(fold+'\\ex2.csv',names=['a','b','c','d','message'],index_col=[4,1]))

--

            a   c   d
message b            
hello   2   1   3   4
world   6   5   7   8
foo     10  9  11  12

(5)

While you could do some munging by hand, the fields here are separated by a variable amount of whitespace. 
In these cases, you can pass a regular expression as a delimiter for read_table. This can be expressed by the regular expression \s+, so we have then:

result = pd.read_table(fold+'\\ex3.txt', sep='\s+')
print(result)

--

            A         B         C
aaa -0.264438 -1.026059 -0.619500
bbb  0.927272  0.302904 -0.032399
ccc -0.264273 -0.386314 -0.217601
ddd -0.871858 -0.348382  1.100491

(6)

import pandas as pd
fold='.\\pydata-book-3rd-edition\\examples'
print(pd.read_csv(fold+'\\ex2.csv',names=['a','b','c','d','message'],skiprows=[0,1]))

--

   a   b   c   d message
0  9  10  11  12     foo

(7)

Handling missing values is an important and frequently nuanced part of the file parsing process. Missing data is usually either not present (empty string) or 
marked by some sentinel value. By default, pandas uses a set of commonly occurring sentinels, such as NA and NULL:

result = pd.read_table(fold+'\\ex5.csv',sep=',')
print(result)
print(pd.isnull(result))

--

  something  a   b     c   d message
0       one  1   2   3.0   4     NaN
1       two  5   6   NaN   8   world
2     three  9  10  11.0  12     foo
   something      a      b      c      d  message
0      False  False  False  False  False     True
1      False  False  False   True  False    False
2      False  False  False  False  False    False

(8)

Make a value in df to null

result = pd.read_table(fold+'\\ex5.csv',sep=',',na_values={'message': ['foo','world'],'something':['two','three']})
print(result)

--

  something  a   b     c   d  message
0       one  1   2   3.0   4      NaN
1       NaN  5   6   NaN   8      NaN
2       NaN  9  10  11.0  12      NaN

(9)

change the display setting to show only amx 10 rows for a dataframe

import pandas as pd
pd.options.display.max_rows = 10

(10)

Data can also be exported to a delimited format. 

result.to_csv(fold+'\\out.csv')

Other delimiters can be used, of course (writing to sys.stdout so it prints the text result to the console)
Missing values appear as empty strings in the output. You might want to denote them by some other sentinel value:

import sys
result.to_csv(sys.stdout,sep='|',index=False,header=False,na_rep='NULL')

--

one|1|2|3.0|4|NULL
NULL|5|6|NULL|8|NULL
NULL|9|10|11.0|12|NULL

(11) JSON Data

JSON (short for JavaScript Object Notation) has become one of the standard formats for sending data by HTTP request between web browsers and other applications. 
It is a much more free-form data format than a tabular text form like CSV. Here is an example:

Json object to python object using loads method and python object to json objects using dumps method.

json_obj="""
{"name":[{"myself":["ankit"],"mylife":"kiio"},{"age":[21,22],"state":["UP","Punjab"]}],
 "Institute":["ISI","IIIT-D"]
}
"""
import json
python_obj=json.loads(json_obj)
print(python_obj)
json_obj=json.dumps(python_obj)
print(json_obj)

--

{'name': [{'myself': ['ankit'], 'mylife': 'kiio'}, {'age': [21, 22], 'state': ['UP', 'Punjab']}], 'Institute': ['ISI', 'IIIT-D']}
{"name": [{"myself": ["ankit"], "mylife": "kiio"}, {"age": [21, 22], "state": ["UP", "Punjab"]}], "Institute": ["ISI", "IIIT-D"]}

(see the difference between single quote and double quote)

(12)

The pandas.read_json can automatically convert JSON datasets in specific arrangements into a Series or DataFrame. For example:

data = pd.read_json(fold+'\\example.json')
data

--


	a	b	c
0	1	2	3
1	4	5	6
2	7	8	9

(13)

If you need to export data from pandas to JSON, one way is to use the to_json methods on Series and DataFrame:

data.to_json('sample.json')
pd.read_json('sample.json')

--

	a	b	c
0	1	2	3
1	4	5	6
2	7	8	9

(14) XML and HTML: Web Scraping:


Python has many libraries for reading and writing data in the ubiquitous HTML and XML formats. Examples include lxml, Beautiful Soup, and html5lib. 
While lxml is comparatively much faster in general, the other libraries can better handle malformed HTML or XML files.

pandas has a built-in function, read_html, which uses libraries like lxml and Beautiful Soup to automatically parse tables out of HTML files as DataFrame objects. 

The pandas.read_html function has a number of options, but by default it searches for and attempts to parse all tabular data contained within

tags. The result is a list of DataFrame objects:

tables=pd.read_html('table.html')
print(tables[0])

--

 Bank NameBank           CityCity StateSt  CertCert  \
0                      Citizens Bank           Sac City      IA      8758   
1           Heartland Tri-State Bank            Elkhart      KS     25851   
2                First Republic Bank      San Francisco      CA     59017   
3                     Signature Bank           New York      NY     57053   
4                Silicon Valley Bank        Santa Clara      CA     24735   
5                  Almena State Bank             Almena      KS     15426   
6         First City Bank of Florida  Fort Walton Beach      FL     16748   
7               The First State Bank      Barboursville      WV     14361   
8                 Ericson State Bank            Ericson      NE     18265   
9   City National Bank of New Jersey             Newark      NJ     21111   
10                     Resolute Bank             Maumee      OH     58317   
11             Louisa Community Bank             Louisa      KY     58112   
 
(15)

time=pd.to_datetime(tables[0]['Closing DateClosing'])
print(time.dt.year.value_counts())

--

2023    5
2020    4
2019    3
Name: Closing DateClosing, dtype: int64

(16)

XML (eXtensible Markup Language) is another common structured data format supporting hierarchical, nested data with metadata. 

from lxml import objectify
parsed=objectify.parse(open('Performance_MNR.xml'))
root=parsed.getroot()
xml_data=[]
skip_fields=['PARENT_SEQ','INDICATOR_SEQ','DESIRED_CHANGE','DECIMAL_PLACES']
for ele in root.INDICATOR:
    data={}
    for child in ele.getchildren():
        if child.tag in skip_fields:
            continue
        else:
            data[child.tag]=child.pyval
    xml_data.append(data)
df=pd.DataFrame(xml_data)
df

--

	AGENCY_NAME	INDICATOR_NAME	DESCRIPTION	PERIOD_YEAR	PERIOD_MONTH	CATEGORY	FREQUENCY	INDICATOR_UNIT	YTD_TARGET	YTD_ACTUAL	MONTHLY_TARGET	MONTHLY_ACTUAL
0	Metro-North Railroad	Escalator Availability	Percent of the time that escalators are operat...	2011	12	Service Indicators	M	%	97.0		97.0	
​
(17)

XML data can get much more complicated than this example. Each tag can have metadata, too. Consider an HTML link tag, which is also valid XML:

from io  import StringIO
root=objectify.parse(StringIO('<a href="http://www.google.com">Google</a>')).getroot()
print(root.get("href"))
print(root.text)

--

http://www.google.com
Google

(18)

One of the easiest ways to store data (also known as serialization) efficiently in binary format is using Python’s built-in pickle serialization. 
pandas objects all have a to_pickle method that writes the data to disk in pickle format:

df.to_pickle('serialized_dataframe')
pd.read_pickle('serialized_dataframe')

--


	AGENCY_NAME	INDICATOR_NAME	DESCRIPTION	PERIOD_YEAR	PERIOD_MONTH	CATEGORY	FREQUENCY	INDICATOR_UNIT	YTD_TARGET	YTD_ACTUAL	MONTHLY_TARGET	MONTHLY_ACTUAL
0	Metro-North Railroad	Escalator Availability	Percent of the time that escalators are operat...	2011	12	Service Indicators	M	%	97.0		97.0	
​
(19)

HDF5 is a well-regarded file format intended for storing large quantities of scientific array data. It is available as a C library, and it has interfaces 
available in many other languages, including Java, Julia, MATLAB, and Python. The “HDF” in HDF5 stands for hierarchical data format. Each HDF5 file can store 
multiple datasets and supporting metadata. Compared with simpler formats, HDF5 supports on-the-fly compression with a variety of compression modes, 
enabling data with repeated patterns to be stored more efficiently. 
HDF5 can be a good choice for working with very large datasets that don’t fit into memory, as you can efficiently read and write small sections of much larger arrays.

While it’s possible to directly access HDF5 files using either the PyTables or h5py libraries, pandas provides a high-level interface that simplifies storing 
Series and DataFrame object. The HDFStore class works like a dict and handles the low-level details:

!conda install pytables
!pip install --user tables

import numpy as np
frame = pd.DataFrame({'a': np.random.randn(100)})
store = pd.HDFStore('mydata.h5')
store['obj1'] = frame
store['obj1_col'] = frame['a']
print(store['obj1'])

--

           a
0  -0.505008
1   0.364013
2   0.461533
3  -0.651221
4  -0.251317
..       ...
95  0.342657
96  1.926088
97 -0.899588
98 -0.970420
99 -1.313902

[100 rows x 1 columns]

(20)

HDFStore supports two storage schemas, 'fixed' and 'table'. The latter is generally slower, but it supports query operations using a special syntax:

store.put('obj2', frame, format='table')
store.select('obj2', where=['index >= 10 and index <= 15'])

--


	a
10	-0.669056
11	1.552245
12	-0.357852
13	1.814004
14	1.462315
15	0.585812

​
(21)

list(store.items())

--

[('/obj1',
  /obj1 (Group) ''
    children := ['axis0' (Array), 'axis1' (Array), 'block0_values' (Array), 'block0_items' (Array)]),
 ('/obj1_col',
  /obj1_col (Group) ''
    children := ['index' (Array), 'values' (Array)]),
 ('/obj2',
  /obj2 (Group) ''
    children := ['table' (Table)]),
 ('/obj3',
  /obj3 (Group) ''
    children := ['table' (Table)])]

store.close()

(22)

The put is an explicit version of the store['obj2'] = frame method but allows us to set other options like the storage format.

The pandas.read_hdf function gives you a shortcut to these tools:

frame.to_hdf('mydata.h5', 'obj3', format='table')
pd.read_hdf('mydata.h5', 'obj3', where=['index < 5'])

	a
0	-1.092360
1	-2.203172
2	-1.035297
3	0.046597
4	0.762753

(23)

If you are processing data that is stored on remote servers, like Amazon S3 or HDFS, using a different binary format designed for distributed storage like 
Apache Parquet may be more suitable.Python for Parquet and other such storage formats is still developing.

If you work with large quantities of data locally, I would encourage you to explore PyTables and h5py to see how they can suit your needs. Since many 
data analysis problems are I/O-bound (rather than CPU-bound), using a tool like HDF5 can massively accelerate your applications.

HDF5 is not a database. It is best suited for write-once, read-many datasets. While data can be added to a file at any time, 
if multiple writers do so simultaneously, the file can become corrupted.

(24)

pandas also supports reading tabular data stored in Excel 2003 (and higher) files using either the ExcelFile class or pandas.read_excel function. 
Internally these tools use the add-on packages xlrd and openpyxl to read XLS and XLSX files, respectively.

To use ExcelFile, create an instance by passing a path to an xls or xlsx file:

# !pip install xlrd==1.1.0
# !pip install openpyxl

xlsx=pd.ExcelFile(fold+'\\ex1.xlsx')
pd.read_excel(xlsx,sheet_name='Sheet1')

--

Unnamed: 0	a	b	c	d	message
0	0	1	2	3	4	hello
1	1	5	6	7	8	world
2	2	9	10	11	12	foo

(25)

To write pandas data to Excel format, you must first create an ExcelWriter, then write data to it using pandas objects’ to_excel method:

writer=pd.ExcelWriter(fold+'\\ex1.xlsx')
frame=pd.DataFrame(np.random.randn(5),columns=['Random Numbers'])
frame.to_excel(writer,sheet_name='Sheet2')
writer.save()

(26)  Interacting with Web APIs

Many websites have public APIs providing data feeds via JSON or some other format. There are a number of ways to access these APIs from Python; 
one easy-to-use method that I recommend is the requests package.

we can make a GET HTTP request using the add-on requests library

import requests
response=requests.get('https://www.weatherapi.com/')
print(response)

-- <Response [200]>

The Response object’s json method will return a dictionary containing JSON parsed into native Python objects:

import requests
response=requests.get('http://ergast.com/api/f1/2004/1/results.json')
print(response)

-- <Response [200]>

data=response.json()
print(data['MRData']['xmlns'])

-- http://ergast.com/mrd/1.5

pd.DataFrame(data).loc['RaceTable']['MRData']

(27) Databases:

import sqlite3
query = """CREATE TABLE test (a VARCHAR(20), b VARCHAR(20), c REAL, d INTEGER );"""
con = sqlite3.connect('mydata.sqlite')
con.execute(query)
con.commit()
data = [('Atlanta', 'Georgia', 1.25, 6),('Tallahassee', 'Florida', 2.6, 3),('Sacramento', 'California', 1.7, 5)]
stmt = "INSERT INTO test VALUES(?, ?, ?, ?)"
con.executemany(stmt, data)
con.commit()
cursor = con.execute('select * from test')
rows = cursor.fetchall()
rows

--

[('Atlanta', 'Georgia', 1.25, 6),
 ('Tallahassee', 'Florida', 2.6, 3),
 ('Sacramento', 'California', 1.7, 5)]


(28)

cursor.description

--

(('a', None, None, None, None, None, None),
 ('b', None, None, None, None, None, None),
 ('c', None, None, None, None, None, None),
 ('d', None, None, None, None, None, None))

pd.DataFrame(rows, columns=[x[0] for x in cursor.description])

--

	a	b	c	d
0	Atlanta	Georgia	1.25	6
1	Tallahassee	Florida	2.60	3
2	Sacramento	California	1.70	5

(29)

The SQLAlchemy project is a popular Python SQL toolkit that abstracts away many of the common differences between SQL databases. 
pandas has a read_sql function that enables you to read data easily from a general SQLAlchemy connection. 
Here, we’ll connect to the same SQLite database with SQLAlchemy and read data from the table created before:

import sqlalchemy as sqla
db = sqla.create_engine('sqlite:///mydata.sqlite')
pd.read_sql('select * from test', db)

--

	a	b	c	d
0	Atlanta	Georgia	1.25	6
1	Tallahassee	Florida	2.60	3
2	Sacramento	California	1.70	5

(30)

import urllib.request
import json 

# Bitcoin Genesis Block Transactions
your_url = 'https://blockchain.info/rawaddr/12c6DSiU4Rq3P4ZxziKxzrL5LmMBrzjrJX'

with urllib.request.urlopen(your_url) as url:
    data = json.loads(url.read().decode())
    print(data)

----------------------------------------------------------------------------------------------------------------------------------------

(31)

import pandas as pd 
import numpy as np
ser=pd.Series(('ankit','kiio',np.nan,'summi'),index=['a','b','c','d'])
print(ser)

--

a    ankit
b     kiio
c      NaN
d    summi
dtype: object

print(ser.isnull())
print(ser.isna())
print(ser.isnull().sum())

--

a    False
b    False
c     True
d    False
dtype: bool
a    False
b    False
c     True
d    False
dtype: bool
1

(32)

print(ser.dropna(axis=0)) # any row with np.nan
print(ser[ser.notnull()])

--

a    ankit
b     kiio
d    summi
dtype: object
a    ankit
b     kiio
d    summi
dtype: object

(33)

df=pd.DataFrame([['ankit',np.nan,'ISI'],['soumi',100,np.nan],['summi',100,'ECIL'],[np.nan,np.nan,np.nan],['kiio',100,'IIIT-D']],columns=['Name','Marks','Institute'],index=['a','b','c','d','e'])
print(df)
print(df.dropna(how='all',axis=0)) # drop all rows with null
print(df.dropna(thresh=3)) # rows with atleast 3 np.nan values

--

    Name  Marks Institute
a  ankit    NaN       ISI
b  soumi  100.0       NaN
c  summi  100.0      ECIL
d    NaN    NaN       NaN
e   kiio  100.0    IIIT-D
    Name  Marks Institute
a  ankit    NaN       ISI
b  soumi  100.0       NaN
c  summi  100.0      ECIL
e   kiio  100.0    IIIT-D
    Name  Marks Institute
c  summi  100.0      ECIL
e   kiio  100.0    IIIT-D

(34)

Duplicate Removal of rows:

data = pd.DataFrame({'k1': ['one', 'two'] * 3 + ['two'],'k2': [1, 1, 2, 3, 3, 4, 4]})
print(data)
print(data.duplicated())
print(data.drop_duplicates())
print(data.drop_duplicates(['k1']))
print(data.drop_duplicates(['k1', 'k2'], keep='last'))

--

    k1  k2
0  one   1
1  two   1
2  one   2
3  two   3
4  one   3
5  two   4
6  two   4
0    False
1    False
2    False
3    False
4    False
5    False
6     True
dtype: bool
    k1  k2
0  one   1
1  two   1
2  one   2
3  two   3
4  one   3
5  two   4
    k1  k2
0  one   1
1  two   1
    k1  k2
0  one   1
1  two   1
2  one   2
3  two   3
4  one   3
6  two   4

​(35)

Transformation:

data=pd.DataFrame(['ankit','kiio','summi'],columns=['Name'],index=['a','b','c'])
print(data)
data['Marks']=data['Name'].map({k:(0 if k=='ankit' else 100) for k in data['Name'] })
print(data)
data['Marks']=data['Name'].map(lambda x:0 if x=='ankit' else 100)
print(data)

--

    Name
a  ankit
b   kiio
c  summi
    Name  Marks
a  ankit      0
b   kiio    100
c  summi    100
    Name  Marks
a  ankit      0
b   kiio    100
c  summi    100

(36)

data=pd.Series([1,999,34,21])
data=data.replace([999,34],np.nan)
print(data)

--

0     1.0
1     NaN
2     NaN
3    21.0
dtype: float64

(37)

data=pd.DataFrame(['ankit','kiio','summi'],columns=['Name'],index=['a','b','c'])
print(data)
data['Name'].replace('summi','soumi',inplace=True)
print(data)

--

    Name
a  ankit
b   kiio
c  summi
    Name
a  ankit
b   kiio
c  soumi

(38)

data=pd.DataFrame(['ankit','kiio','summi'],columns=['Name'],index=['abc','bcd','cde'])
data.rename(index=str.capitalize,columns=str.upper)

--


	NAME
Abc	ankit
Bcd	kiio
Cde	summi

(39)

data.rename(index={'abc':'new','cde':'new_index'},columns={'Name':'New_Name'},inplace=True)
print(data)

--

          New_Name
new          ankit
bcd           kiio
new_index    summi

(40)

Making bins of data

b=pd.cut([12,2,1,22,11,34,32,1,41,48,45,51,58,61,66,67,69,70,71,72,90,91,81,81,11,2,3,2,1,2,0],[0,10,20,30,40,50,60,70,80,90,100],right=True,.
labels=['0-10','11-20','21-30','31-40','41-50','51-60','61-70','71-80','81-90','91-100'])
print(b)
print(b.codes)
print(b.categories)
print(pd.value_counts(b))

--

['11-20', '0-10', '0-10', '21-30', '11-20', ..., '0-10', '0-10', '0-10', '0-10', NaN]
Length: 31
Categories (10, object): ['0-10' < '11-20' < '21-30' < '31-40' ... '61-70' < '71-80' < '81-90' < '91-100']
[ 1  0  0  2  1  3  3  0  4  4  4  5  5  6  6  6  6  6  7  7  8  9  8  8
  1  0  0  0  0  0 -1]
Index(['0-10', '11-20', '21-30', '31-40', '41-50', '51-60', '61-70', '71-80',
       '81-90', '91-100'],
      dtype='object')
0-10      8
61-70     5
11-20     3
41-50     3
81-90     3
31-40     2
51-60     2
71-80     2
21-30     1
91-100    1
dtype: int64

(41)

A closely related function, qcut, bins the data based on sample quantiles. Depending on the distribution of the data, using cut will 
not usually result in each bin having the same number of data points. Since qcut uses sample quantiles instead, by definition you will obtain roughly equal-size bins:

data = np.random.randn(1000) # Normally distributed
cats = pd.qcut(data, 4) # Cut into quartiles
print(cats)
print(pd.value_counts(cats))

--

[(0.0084, 0.626], (0.626, 3.354], (0.0084, 0.626], (0.626, 3.354], (0.0084, 0.626], ..., (0.626, 3.354], (0.626, 3.354], (0.626, 3.354], (0.0084, 0.626], (0.626, 3.354]]
Length: 1000
Categories (4, interval[float64, right]): [(-3.62, -0.691] < (-0.691, 0.0084] < (0.0084, 0.626] < (0.626, 3.354]]
(-3.62, -0.691]     250
(-0.691, 0.0084]    250
(0.0084, 0.626]     250
(0.626, 3.354]      250
dtype: int64

(42)

Similar to cut you can pass your own quantiles (numbers between 0 and 1, inclusive):

pd.qcut(data, [0, 0.1, 0.5, 0.9, 1.])

--

[(-1.308, -0.009], (-0.009, 1.192], (-1.308, -0.009], (-1.308, -0.009], (-0.009, 1.192], ..., (-0.009, 1.192], (-1.308, -0.009], (-0.009, 1.192], (-0.009, 1.192], (-3.895, -1.308]]
Length: 1000
Categories (4, interval[float64]): [(-3.895, -1.308] < (-1.308, -0.009] < (-0.009, 1.192] < (1.192, 2.635]]

(43)

Detecting and Filtering Outliers

Suppose you wanted to find values in one of the columns exceeding 3 in absolute value:

data = pd.DataFrame(np.random.randn(1000, 4))
data[(np.abs(data) > 0).any(1)]
data[np.abs(data) > 3] = np.sign(data) * 3
print(data)
# The statement np.sign(data) produces 1 and –1 values based on whether the values in data are positive or negative:

--

            0         1         2         3
0    0.555229 -0.063679  0.322598  1.370509
1    0.688280 -0.770756 -1.974387 -0.942000
2   -1.254370  0.097759 -0.186317  0.362924
3   -0.634275  1.443248  1.699919  0.492581
4    1.529708  2.928737  1.146503 -1.391692
..        ...       ...       ...       ...
995  0.524295  0.838711  1.587841  0.080143
996 -0.466875  1.822928 -0.349306 -0.968938
997 -0.684669  0.195257 -1.432022 -0.364939
998 -0.559563 -0.596628  2.880303 -0.281358
999 -0.075537  0.024840 -0.358946  0.586875

[1000 rows x 4 columns]

(44)

df = pd.DataFrame(np.arange(5 * 4).reshape((5, 4)),columns=['a','b','c','d'])
print(df)
sampler=np.random.permutation(5)
print(df.take(sampler,axis=0))
# To select a random subset without replacement, you can use the sample method on Series and DataFrame:
print(df.sample(n=3))
#To generate a sample with replacement (to allow repeat choices), pass replace=True to sample:

choices = pd.Series([5, 7, -1, 6, 4])
draws = choices.sample(n=10, replace=True)
print(draws)

--

    a   b   c   d
0   0   1   2   3
1   4   5   6   7
2   8   9  10  11
3  12  13  14  15
4  16  17  18  19
    a   b   c   d
1   4   5   6   7
3  12  13  14  15
2   8   9  10  11
4  16  17  18  19
0   0   1   2   3
    a   b   c   d
3  12  13  14  15
0   0   1   2   3
4  16  17  18  19
0    5
4    4
0    5
1    7
3    6
1    7
1    7
1    7
4    4
0    5
dtype: int64

(45)

categorical to dummy variables:

df=pd.DataFrame({'Name':['ankit','kiio','summi'],'Marks':range(3)})
print(df)
print(pd.get_dummies(df['Name'],prefix='name').join(df[['Marks']]))

--

    Name  Marks
0  ankit      0
1   kiio      1
2  summi      2

b=pd.cut([12,2,1,22,11,34,32,1,41,48,45,51,58,61,66,67,69,70,71,72,90,91,81,81,11,2,3,2,1,2,0],[0,10,20,30,40,50,60,70,80,90,100],
right=True,labels=['0-10','11-20','21-30','31-40','41-50','51-60','61-70','71-80','81-90','91-100'])
print(pd.get_dummies(b))

--

    0-10  11-20  21-30  31-40  41-50  51-60  61-70  71-80  81-90  91-100
0      0      1      0      0      0      0      0      0      0       0
1      1      0      0      0      0      0      0      0      0       0
2      1      0      0      0      0      0      0      0      0       0
3      0      0      1      0      0      0      0      0      0       0
4      0      1      0      0      0      0      0      0      0       0
5      0      0      0      1      0      0      0      0      0       0
6      0      0      0      1      0      0      0      0      0       0
7      1      0      0      0      0      0      0      0      0       0
8      0      0      0      0      1      0      0      0      0       0
9      0      0      0      0      1      0      0      0      0       0
10     0      0      0      0      1      0      0      0      0       0
11     0      0      0      0      0      1      0      0      0       0
12     0      0      0      0      0      1      0      0      0       0
13     0      0      0      0      0      0      1      0      0       0
14     0      0      0      0      0      0      1      0      0       0
15     0      0      0      0      0      0      1      0      0       0
16     0      0      0      0      0      0      1      0      0       0
17     0      0      0      0      0      0      1      0      0       0
18     0      0      0      0      0      0      0      1      0       0
19     0      0      0      0      0      0      0      1      0       0
20     0      0      0      0      0      0      0      0      1       0
21     0      0      0      0      0      0      0      0      0       1
22     0      0      0      0      0      0      0      0      1       0
23     0      0      0      0      0      0      0      0      1       0
24     0      1      0      0      0      0      0      0      0       0
25     1      0      0      0      0      0      0      0      0       0
26     1      0      0      0      0      0      0      0      0       0
27     1      0      0      0      0      0      0      0      0       0
28     1      0      0      0      0      0      0      0      0       0
29     1      0      0      0      0      0      0      0      0       0
30     0      0      0      0      0      0      0      0      0       0

(46)

String Manipulation:

"::".join([s.strip() for s  in 'ankit ,   kiio ,  summi'.split(',')])

--

'ankit::kiio::summi'

(47)

print([1,2,3].index(3))
print("ankit".index('k'))

--

2
2

(48)

"::".join([s.strip() for s  in 'ankit ,   kiio ,  summi'.split(',')]).find(':')

-- 5

Note the difference between find and index is that index raises an exception if the string isn’t found (versus returning –1)

"::".join([s.strip() for s  in 'ankit ,   kiio ,  summi'.split(',')]).count(':')

-- 4

(49)

"::".join([s.strip() for s  in 'ankit ,   kiio ,  summi'.split(',')]).replace('::','@')

-- 'ankit@kiio@summi'

(50)

Argument --> Description

count --> Return the number of non-overlapping occurrences of substring in the string.

endswith --> Returns True if string ends with sux.

startswith --> Returns True if string starts with prefix.

join --> Use string as delimiter for concatenating a sequence of other strings.

index --> Return position of first character in substring if found in the string; raises ValueError if not found.

find --> Return position of first character of rst occurrence of substring in the string; like index, but returns –1 if not found.

rfind --> Return position of first character of last occurrence of substring in the string; returns –1 if not found.

replace --> Replace occurrences of string with another string.

strip,rstrip,lstrip --> Trim whitespace, including newlines; equivalent to x.strip() (and rstrip, lstrip, respectively) for each element.

split --> Break string into list of substrings using passed delimiter.

lower --> Convert alphabet characters to lowercase.

upper --> Convert alphabet characters to uppercase.

casefold --> Convert characters to lowercase, and convert any region-specific variable character combinations to a common comparable form.

ljust,rjust --> Left justify or right justify, respectively; pad opposite side of string with spaces (or some other fill character) to return a string with a 
                minimum width.

(51)

Regular Expressions:

The re module functions fall into three categories: pattern matching, substitution, and splitting. Naturally these are all related; a regex describes a pattern to 
locate in the text, which can then be used for many purposes. Let’s look at a simple example:

(52)

suppose we wanted to split a string with a variable number of whitespace characters (tabs, spaces, and newlines). 
The regex describing one or more whitespace characters is \s+:

import re 
text="ankit\n summi\t\t\r kiio"
re.split('\s+',text)

--

['ankit', 'summi', 'kiio']

When you call re.split('\s+', text), the regular expression is first compiled, and then its split method is called on the passed text. 
You can compile the regex yourself with re.compile, forming a reusable regex object:

regex=re.compile('\s+')
regex.split(text)

--

['ankit', 'summi', 'kiio']

(53)

If, instead, you wanted to get a list of all patterns matching the regex, you can use the findall method:

regex.findall(text)

--

['\n ', '\t\t\r ']

To avoid unwanted escaping with \ in a regular expression, use raw string literals like r'C:\x' instead of the equivalent 'C:\x'.

Creating a regex object with re.compile is highly recommended if you intend to apply the same expression to many strings; doing so will save CPU cycles.

(54)

text = """Dave dave@google.com
Steve steve@gmail.com
Rob rob@gmail.com
Ryan ryan@yahoo.com
"""
     
pattern = r'[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,4}'
      
# re.IGNORECASE makes the regex case-insensitive
regex = re.compile(pattern, flags=re.IGNORECASE)
regex

--

re.compile(r'[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,4}', re.IGNORECASE|re.UNICODE)

(55)

regex.findall(text)

--

['dave@google.com', 'steve@gmail.com', 'rob@gmail.com', 'ryan@yahoo.com']

(56)

search returns a special match object for the first email address in the text. For the preceding regex, 
the match object can only tell us the start and end position of the pattern in the string:

m = regex.search(text)
m

--

<_sre.SRE_Match object; span=(5, 20), match='dave@google.com'>

text[m.start():m.end()]

--

'dave@google.com'

(57)

Relatedly, sub will return a new string with occurrences of the pattern replaced by the a new string:

print(regex.sub('REDACTED', text))

--

Dave REDACTED
Steve REDACTED
Rob REDACTED
Ryan REDACTED

(58)

Suppose you wanted to find email addresses and simultaneously segment each address into its three components: username, domain name, and domain suffix. 
To do this, put parentheses around the parts of the pattern to segment:

pattern = r'([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\.([A-Z]{2,4})'
regex = re.compile(pattern, flags=re.IGNORECASE)

A match object produced by this modified regex returns a tuple of the pattern components with its groups method:

m = regex.match('wesm@bright.net')
m.groups()

--

('wesm', 'bright', 'net')

(59)

findall returns a list of tuples when the pattern has groups:

regex.findall(text)

--

[('dave', 'google', 'com'),
 ('steve', 'gmail', 'com'),
 ('rob', 'gmail', 'com'),
 ('ryan', 'yahoo', 'com')]

sub also has access to groups in each match using special symbols like \1 and \2. 
The symbol \1 corresponds to the first matched group, \2 corresponds to the second, and so forth:

print(regex.sub(r'Username: \1, Domain: \2, Suffix: \3', text))

--


Dave Username: dave, Domain: google, Suffix: com
Steve Username: steve, Domain: gmail, Suffix: com
Rob Username: rob, Domain: gmail, Suffix: com
Ryan Username: ryan, Domain: yahoo, Suffix: com

(60)

Table 7-4. Regular expression methods
Argument ---> Description

findall ---> Return all non-overlapping matching patterns in a string as a list

finditer ---> Like findall, but returns an iterator

match ---> Match pattern at start of string and optionally segment pattern components into groups; if the pattern matches, returns a match object, and otherwise None

search ---> Scan string for match to pattern; returning a match object if so; unlike match, the match can be anywhere in the string as opposed to only at the beginning

split ---> Break string into pieces at each occurrence of pattern

sub, subn ---> Replace all (sub) or first n occurrences (subn) of pattern in string with replacement expression; use symbols \1, \2, ... 
to refer to match group elements in the replacement string

(61)

Vectorized String Functions in pandas:

data = {'Dave': 'dave@google.com', 'Steve': 'steve@gmail.com','Rob': 'rob@gmail.com', 'Wes': np.nan}
data = pd.Series(data)
data

--

Dave     dave@google.com
Steve    steve@gmail.com
Rob        rob@gmail.com
Wes                  NaN
dtype: object

(62)

data.str.contains('gmail')

--

Dave     False
Steve     True
Rob       True
Wes        NaN
dtype: object

(63)

data.str.findall(pattern, flags=re.IGNORECASE)

--

Dave     [(dave, google, com)]
Steve    [(steve, gmail, com)]
Rob        [(rob, gmail, com)]
Wes                        NaN
dtype: object

(64)

matches = data.str.match(pattern, flags=re.IGNORECASE)
matches

--

Dave     True
Steve    True
Rob      True
Wes       NaN
dtype: object

(65)

data.str[:5]

--

Dave     dave@
Steve    steve
Rob      rob@g
Wes        NaN
dtype: object

(66)

Table 7-5. Partial listing of vectorized string methods
Method --> Description

cat --> Concatenate strings element-wise with optional delimiter

contains --> Return boolean array if each string contains pattern/regex

count --> Count occurrences of pattern

extract --> Use a regular expression with groups to extract one or more strings from a Series of strings; the result will be a DataFrame with one column per group

endswith --> Equivalent to x.endswith(pattern) for each element

startswith --> Equivalent to x.startswith(pattern) for each element

findall --> Compute list of all occurrences of pattern/regex for each string

get --> Index into each element (retrieve i-th element)

isalnum --> Equivalent to built-in str.alnum

isalpha --> Equivalent to built-in str.isalpha

isdecimal --> Equivalent to built-in str.isdecimal

isdigit --> Equivalent to built-in str.isdigit

islower --> Equivalent to built-in str.islower

isnumeric --> Equivalent to built-in str.isnumeric

isupper --> Equivalent to built-in str.isupper

join --> Join strings in each element of the Series with passed separator

len --> Compute length of each string

lower, upper --> Convert cases; equivalent to x.lower() or x.upper() for each element

match --> Use re.match with the passed regular expression on each element, returning matched groups as list

pad --> Add whitespace to left, right, or both sides of strings

center --> Equivalent to pad(side='both')

repeat --> Duplicate values (e.g., s.str.repeat(3) is equivalent to x * 3 for each string)

replace --> Replace occurrences of pattern/regex with some other string

slice --> Slice each string in the Series

split --> Split strings on delimiter or regular expression

strip --> Trim whitespace from both sides, including newlines

rstrip --> Trim whitespace on right side

lstrip --> Trim whitespace on left side