(https://www.geeksforgeeks.org/nlp-interview-questions/)

(1) NLP encompasses a wide range of tasks, including language translation, sentiment analysis,
 text categorization, information extraction, speech recognition, and natural language understanding.

(2) In NLP, a corpus is a huge collection of texts or documents. It is a structured dataset.

(3) Text augmentation in NLP refers to the process that generates new or modified textual data
   from existing data in order to increase the diversity and quantity of training samples.

(4) Different text augmentation techniques in NLP include:

Synonym Replacement
Random Insertion/Deletion
Word Swapping
Back translation
Random Masking
Character-level Augmentation
Rule-based Generation
Text Paraphrasing

(5) 

Preprocessing in NLP typically involves a series of steps, which may include:

Tokenization
Stop Word Removal
Text Normalization
  Lowercasing
  Lemmatization
  Stemming
  Date and Time Normalization
Removal of Special Characters and Punctuation
Removing HTML Tags or Markup
Spell Correction
Sentence Segmentation

(6) 

In stemming, the word suffixes are removed using the heuristic or pattern-based rules regardless of the context of the parts of speech.
The resulting stems may not always be actual dictionary words. Stemming algorithms are generally simpler and faster compared to lemmatization,
making them suitable for certain applications with time or resource constraints.

In lemmatization, The root form of the word known as lemma, is determined by considering the word’s context and parts of speech.
It uses linguistic knowledge and databases (e.g., wordnet) to transform words into their root form. In this case,
the output lemma is a valid word as per the dictionary. For example, lemmatizing “running”
and “runner” would result in “run.” Lemmatization provides better interpretability and can be more accurate for tasks that require meaningful word representations.

(7)

There are three main approaches for POS tagging:

Rule-based POS tagging:

It uses a set of handcrafted rules to determine the part of speech based on morphological, syntactic, and contextual patterns for each word in a sentence.
For example, words ending with ‘-ing’ are likely to be a verb.

Statistical POS tagging:

The statistical model like Hidden Markov Model (HMMs) or Conditional Random Fields (CRFs) are trained on a large corpus of already tagged text.
The model learns the probability of word sequences with their corresponding POS tags, and it can be further used for assigning each word to a most likely POS tag based
on the context in which the word appears.

Neural network POS tagging:
 
The neural network-based model like RNN, LSTM, Bi-directional RNN, and transformer have given promising results in POS tagging by learning the patterns and
representations of words and their context.

(8)

Named Entity Recognization (NER) is a task in natural language processing that is used to identify and classify the named entity in text.
Named entity refers to real-world objects or concepts, such as persons, organizations, locations, dates, etc.

(9)

The goal of NER is to extract and classify these named entities in order to offer structured data about the entities referenced in a given text.

(10)

In NLP, parsing is defined as the process of determining the underlying structure of a sentence by breaking it down into constituent parts and
determining the syntactic relationships between them according to formal grammar rules.
The purpose of parsing is to understand the syntactic structure of a sentence, which allows for deeper learning of its meaning and encourages different
downstream NLP tasks such as semantic analysis, information extraction, question answering, and machine translation.
it is also known as syntax analysis or syntactic parsing.

The formal grammar rules used in parsing are typically based on Chomsky’s hierarchy.

(11)

In natural language processing (NLP), there are several types of parsing algorithms used to analyze the grammatical structure of sentences.
Here are some of the main types of parsing algorithms:

Constituency Parsing:

Constituency parsing in NLP tries to figure out a sentence’s hierarchical structure by breaking it into constituents based on a particular grammar.
It generates valid constituent structures using context-free grammar. The parse tree that results represents the structure of the sentence,
with the root node representing the complete sentence and internal nodes representing phrases.
Constituency parsing techniques like as CKY, Earley, and chart parsing are often used for parsing.
This approach is appropriate for tasks that need a thorough comprehension of sentence structure, such as semantic analysis and machine translation.
When a complete understanding of sentence structure is required, constituency parsing, a classic parsing approach, is applied.

Dependency Parsing:

In NLP, dependency parsing identifies grammatical relationships between words in a sentence.
It represents the sentence as a directed graph, with dependencies shown as labelled arcs.
The graph emphasises subject-verb, noun-modifier, and object-preposition relationships. 
Dependency parsing, as opposed to constituency parsing, is helpful for languages with flexible word order.

Top-down parsing

Bottom-up parsing

(12)

In natural language processing (NLP), A vector space is a mathematical vector where words or documents are represented by numerical vectors form.
Vector space models are used to convert text into numerical representations that machine learning algorithms can understand.
Vector spaces are generated using techniques such as word embeddings, bag-of-words, and term frequency-inverse document frequency (TF-IDF).

These methods allow for the conversion of textual data into dense or sparse vectors in a high-dimensional space. Each dimension of the vector may indicate
a different feature,such as the presence or absence of a word, word frequency, semantic meaning, or contextual information.

(13)

Bag of Words is a classical text representation technique in NLP that describes the occurrence of words within a document or not.
It just keeps track of word counts and ignores the grammatical details and the word order.

(14)

The Bag of n-grams model is a modification of the standard bag-of-words (BoW) model in NLP.
Instead of taking individual words to be the fundamental units of representation, the Bag of n-grams model considers contiguous sequences of n words,
known as n-grams, to be the fundamental units of representation.

(15)

The steps for creating a bag-of-n-grams model are as follows:

A. The text is split or tokenized into individual words or characters.

B. The tokenized text is used to construct N-grams of size n (sequences of n consecutive words or characters).
If n is set to 1 known as uni-gram i.e. same as a bag of words, 2 i.e. bi-grams, and 3 i.e. tri-gram.

C. A vocabulary is built by collecting all unique n-grams across the entire corpus.

D. Similarly to the BoW approach, each document is represented as a numerical vector. 
The vector’s dimensions correspond to the vocabulary’s unique n-grams, and the value in each dimension denotes the frequency or occurrence of that n-gram
in the document.

(16)

Term frequency-inverse document frequency (TF-IDF) is a classical text representation technique in NLP that uses a statistical measure to evaluate the
importance of a word in a document relative to a corpus of documents.
It is a combination of two terms: term frequency (TF) and inverse document frequency (IDF).

(17)

The similarity between two vectors in a multi-dimensional space is measured using the cosine similarity metric. To determine
how similar or unlike the vectors are to one another, it calculates the cosine of the angle between them.

In natural language processing (NLP), Cosine similarity is used to compare two vectors that represent text.
The degree of similarity is calculated using the cosine of the angle between the document vectors.
To compute the cosine similarity between two text document vectors, we often used the following procedures:

Text Representation:

Convert text documents into numerical vectors using approaches like bag-of-words, TF-IDF (Term Frequency-Inverse Document Frequency), or word embeddings
like Word2Vec or GloVe.

Vector Normalization:

Normalize the document vectors to unit length. This normalization step ensures that the length or magnitude of the vectors does not affect the
cosine similarity calculation.

Cosine Similarity Calculation:

Take the dot product of the normalised vectors and divide it by the product of the magnitudes of the vectors to obtain the cosine similarity.

(18)

The resulting cosine similarity (cos(theta)) score ranges from -1 to 1, where 1 represents the highest similarity, 0 represents no similarity, and -1 represents
the maximum dissimilarity between the documents.

(19)

LSTMs are a type of recurrent neural network that was developed to deal with the vanishing gradient problem of RNN.
LSTMs are useful for capturing long-term dependencies in sequences, and they have been used in applications such as machine translation,
named entity identification, and sentiment analysis.

(20)

Transformers are a relatively recent architecture that has gained significant attention in NLP.
By exploiting self-attention processes to capture contextual relationships in text, transformers such as the
BERT (Bidirectional Encoder Representations from Transformers) model have achieved state-of-the-art performance in a wide range of NLP tasks.

(21)

Sequence labelling is one of the fundamental NLP tasks in which, categorical labels are assigned to each individual element in a sequence. 
The sequence can represent various linguistic units such as words, characters, sentences, or paragraphs.

Sequence labelling in NLP includes the following tasks.

Part-of-Speech Tagging (POS Tagging)
Named Entity Recognition (NER)
Chunking: Words are organized into syntactic units or “chunks” based on their grammatical roles (for example, noun phrase, verb phrase).
Semantic Role Labeling (SRL): In which, words or phrases in a sentence are labelled based on their semantic roles like Teacher, Doctor, Engineer, Lawyer etc
Speech Tagging: In speech processing tasks such as speech recognition or phoneme classification, labels are assigned to phonetic units or acoustic segments.

Machine learning models like Conditional Random Fields (CRFs), Hidden Markov Models (HMMs), recurrent neural networks (RNNs), or
transformers are used for sequence labelling tasks.
These models learn from the labelled training data to make predictions on unseen data.

(22) Topic Modelling in NLP

Where each topic will be distributed over words means each topic is a list of words, and each word has a probability associated with it.
and the words that have the highest probabilities in a topic are the words that are most likely to be used to describe that topic.
For example, the words like “neural”, “RNN”, and “architecture” are the keywords for neural networks and the words like ‘language”, and “sentiment” 
are the keywords for Natural Language processing.

There are a number of topic modelling algorithms but two of the most popular topic modelling algorithms are as follows:

Latent Dirichlet Allocation (LDA): LDA is based on the idea that each text in the corpus is a mash-up of various topics and
that each word in the document is derived from one of those topics. It is assumed that there is an unobservable (latent) set of topics and
each document is generated by Topic Selection or Word Generation.

Non-Negative Matrix Factorization (NMF): NMF is a matrix factorization technique that approximates the term-document matrix
(where rows represent documents and columns represent words) into two non-negative matrices: one representing the topic-word relationships and
the other the document-topic relationships.
NMF aims to identify representative topics and weights for each document.

(23)

GPT stands for “Generative Pre-trained Transformer”. It refers to a collection of large language models created by OpenAI. 
It is trained on a massive dataset of text and code, which allows it to generate text, generate code, translate languages, and
write many types of creative content, as well as answer questions in an informative manner.
The GPT series includes various models, the most well-known and commonly utilised of which are the GPT-2 and GPT-3.

(24)

GPT models are built on the Transformer architecture, which allows them to efficiently capture long-term dependencies and contextual information in text.
These models are pre-trained on a large corpus of text data from the internet,which enables them to learn the underlying patterns and structures of language.

(25)

Word embeddings in NLP are defined as the dense, low-dimensional vector representations of words
that capture semantic and contextual information about words in a language.
It is trained using big text corpora through unsupervised or supervised methods to represent words in a numerical format
that can be processed by machine learning models.

The main goal of Word embeddings is to capture relationships and similarities between words by representing them as dense vectors in a continuous vector space.
These vector representations are acquired using the distributional hypothesis, which states that words with similar meanings tend to occur in similar contexts.
Some of the popular pre-trained word embeddings are Word2Vec, GloVe (Global Vectors for Word Representation), or FastText.

The advantages of word embedding over the traditional text vectorization technique are as follows:

A. It can capture the Semantic Similarity between the words
B. It is capable of capturing syntactic links between words.
   Vector operations such as “king” – “man” + “woman” may produce a vector similar to the vector for “queen,” capturing the gender analogy.
C. Compared to one-shot encoding, it has reduced the dimensionality of word representations.
   Instead of high-dimensional sparse vectors, word embeddings typically have a fixed length and represent words as dense vectors.
D. It can be generalized to represent words that they have not been trained on i.e. out-of-vocabulary words.
   This is done by using the learned word associations to place new words in the vector space near words that they are semantically or syntactically similar to.

(26)

There are various approaches that are typically used for training word embeddings,
which are dense vector representations of words in a continuous vector space. Some of the popular word embedding algorithms are as follows:

Word2Vec:

Word2vec is a common approach for generating vector representations of words that reflect their meaning and relationships.
Word2vec learns embeddings using a shallow neural network and follows two approaches: CBOW and Skip-gram
CBOW (Continuous Bag-of-Words) predicts a target word based on its context words.
Skip-gram predicts context words given a target word.

GloVe:

GloVe (Global Vectors for Word Representation) is a word embedding model that is similar to Word2vec. GloVe, on the other hand,
uses  objective function that constructs a co-occurrence matrix based on the statistics of word co-occurrences in a large corpus

The co-occurrence matrix is a square matrix where each entry represents the number of times two words co-occur in a window of a certain size.
GloVe then performs matrix factorization on the co-occurrence matrix. 
Matrix factorization is a technique for finding a low-dimensional representation of a high-dimensional matrix. 
In the case of GloVe, the low-dimensional representation is a vector representation for each word in the corpus. 
The word embeddings are learned by minimizing a loss function that measures the difference between the predicted co-occurrence probabilities and
the actual co-occurrence probabilities. This makes GloVe more robust to noise and less sensitive to the order of words in a sentence.


FastText:
 
FastText is a Word2vec extension that includes subword information.
It represents words as bags of character n-grams, allowing it to handle out-of-vocabulary terms and capture morphological information.
During training, FastText considers subword information as well as word context..

ELMo:

ELMo is a deeply contextualised word embedding model that generates context-dependent word representations.
It generates word embeddings that capture both semantic and syntactic information based on the context of the word using bidirectional language models.

BERT:

A transformer-based model called BERT (Bidirectional Encoder Representations from Transformers) learns contextualised word embeddings.
BERT is trained on a large corpus by anticipating masked terms inside a sentence and gaining knowledge about the bidirectional context. 

(27)

OOV words are words that are missing in a language model’s vocabulary or the training data it was trained on.

Here are a few approaches to handling OOV words in NLP:

Character-level models:

Character-level models can be used in place of word-level representations.
In this method, words are broken down into individual characters, and the model learns representations based on character sequences.
As a result, the model can handle OOV words since it can generalize from known character patterns.

Subword tokenization:

Byte-Pair Encoding (BPE) and WordPiece are two subword tokenization algorithms that divide words into smaller subword units based on their frequency
in the training data. This method enables the model to handle OOV words by representing them as a combination of subwords that it comes across during training.

Unknown token:
 
Use a special token, frequently referred to as an “unknown” token or “UNK,” to represent any OOV term that appears during inference.
Every time the model comes across an OOV term, it replaces it with the unidentified token and keeps processing.
The model is still able to generate relevant output even though this technique doesn’t explicitly define the meaning of the OOV word. 

External knowledge:

When dealing with OOV terms, using external knowledge resources, like a knowledge graph or an external dictionary, can be helpful.
We need to try to look up a word’s definition or relevant information in the external knowledge source when we come across an OOV word.

Fine-tuning:
 
We can fine-tune using the pre-trained language model with domain-specific or task-specific data that includes OOV words.
By incorporating OOV words in the fine-tuning process, we expose the model to these words and increase its capacity to handle them.

(28)

A character-level language model represents text as a sequence of characters, whereas a word-level language model represents text as a sequence of words.

Word-level language models are often easier to interpret and more efficient to train. They are, however, less accurate than character-level language models
because they cannot capture the intricacies of the text that are stored in the character order. Character-level language models are more accurate than
word-level language models, but they are more complex to train and interpret. They are also more sensitive to noise in the text,
as a slight alteration in a character can have a large impact on the meaning of the text.

(29)

The task of determining which sense of a word is intended in a given context is known as word sense disambiguation (WSD).
This is a challenging task because many words have several meanings that can only be determined by considering the context in which the word is used.

For example, the word “bank” can be used to refer to a variety of things, including “a financial institution,” “a riverbank,” and “a slope.” 
The term “bank” in the sentence “I went to the bank to deposit my money” should be understood to mean “a financial institution.” 
This is so because the sentence’s context implies that the speaker is on their way to a location where they can deposit money.

(30)

Co-reference resolution is a natural language processing (NLP) task that involves identifying all expressions in a text that refer to the same entity. 
In other words, it tries to determine whether words or phrases in a text, typically pronouns or noun phrases, correspond to the same real-world thing. 
For example, the pronoun “he” in the sentence “Pawan Gunjan has compiled this article, He had done lots of research on Various NLP interview questions” 
refers to Pawan Gunjan himself. Co-reference resolution automatically identifies such linkages and establishes that “He” refers to “Pawan Gunjan” in all instances.

(31)

Some of the common techniques used for information extraction include:

Named entity recognition (NER)
Relationship extraction
Coreference resolution
Deep Learning-based Approaches

(32)

Hidden Markov Model is a probabilistic model based on the Markov Chain Rule used for modelling sequential data like
characters, words, and sentences by computing the probability distribution of sequences.

Markov chain uses the Markov assumptions which state that the probabilities future state of the system only depends on its present state, 
not on any past state of the system. 
This assumption simplifies the modelling process by reducing the amount of information needed to predict future states.

The underlying process in an HMM is represented by a set of hidden states that are not directly observable.
Based on the hidden states, the observed data, such as characters, words, or phrases, are generated.

Hidden Markov Models consist of two key components:

Transition Probabilities:

The transition probabilities in Hidden Markov Models(HMMs) represents the likelihood of moving from one hidden state to another. 
It captures the dependencies or relationships between adjacent states in the sequence. 
In part-of-speech tagging, for example, the HMM’s hidden states represent distinct part-of-speech tags, 
and the transition probabilities indicate the likelihood of transitioning from one part-of-speech tag to another.

Emission Probabilities: 

In HMMs, emission probabilities define the likelihood of observing specific symbols (characters, words, etc.) given a particular hidden state. 
The link between the hidden states and the observable symbols is encoded by these probabilities.

Emission probabilities are often used in NLP to represent the relationship between words and linguistic features such as part-of-speech tags or 
other linguistic variables. The HMM captures the likelihood of generating an observable symbol (e.g., word) from a 
specific hidden state (e.g., part-of-speech tag) by calculating the emission probabilities.

(33)

Hidden Markov Models (HMMs) estimate transition and emission probabilities from labelled data using approaches such as the Baum-Welch algorithm. 
Inference algorithms like Viterbi and Forward-Backward are used to determine the most likely sequence of hidden states given observed symbols. 
HMMs are used to represent sequential data and have been implemented in NLP applications such as part-of-speech tagging. 
However, advanced models, such as CRFs and neural networks, frequently beat HMMs due to their flexibility and ability to capture richer dependencies.

(34)

Conditional Random Fields are a probabilistic graphical model that is designed to predict the sequence of labels for a given sequence of observations. 
It is well-suited for prediction tasks in which contextual information or dependencies among neighbouring elements are crucial.

CRFs are an extension of Hidden Markov Models (HMMs) that allow for the modelling of more complex relationships between labels in a sequence. 
It is specifically designed to capture dependencies between non-consecutive labels, whereas HMMs presume a 
Markov property in which the current state is only dependent on the past state. This makes CRFs more adaptable and suitable for capturing long-term dependencies 
and complicated label interactions.

In a CRF model, the labels and observations are represented as a graph. 
The nodes in the graph represent the labels, and the edges represent the dependencies between the labels. 
The model assigns weights to features that capture relevant information about the observations and labels.

During training, the CRF model learns the weights by maximizing the conditional log-likelihood of the labelled training data. 
This process involves optimization algorithms such as gradient descent or the iterative scaling algorithm.

During inference, given an input sequence, the CRF model calculates the conditional probabilities of different label sequences. 
Algorithms like the Viterbi algorithm efficiently find the most likely label sequence based on these probabilities.


CRFs have demonstrated high performance in a variety of sequence labelling tasks like named entity identification, part-of-speech tagging, and others.

(35)

RNN is used as language translation, speech recognition, sentiment analysis, natural language production, summary writing, and so on. 
It differs from feedforward neural networks in that the input data in RNN does not only flow in a single direction but also has a loop or cycle inside its design 
that has “memory” that preserves information over time. As a result, the RNN can handle data where context is critical, such as natural languages.

(36)

Standard RNNs are vulnerable to the vanishing gradient problem, in which gradients decrease exponentially as they propagate backwards through time. 
Because of this issue, it is difficult for the network to capture and transmit long-term dependencies across multiple time steps during training.

RNNs, on the other hand, can suffer from the expanding gradient problem, in which gradients get exceedingly big and cause unstable training. 
This issue can cause the network to converge slowly or fail to converge at all.

Standard RNNs have limited memory and fail to remember information from previous time steps. Because of this limitation, 
they have difficulty capturing long-term dependencies in sequences, limiting their ability to model complicated relationships that span a significant number of 
time steps.

(37)

A Long Short-Term Memory (LSTM) network is a type of recurrent neural network (RNN) architecture that is designed to solve the vanishing gradient problem 
and capture long-term dependencies in sequential data. LSTM networks are particularly effective in tasks that involve processing and understanding sequential data, 
such as natural language processing and speech recognition.

The key idea behind LSTMs is the integration of a memory cell, which acts as a memory unit capable of retaining information for an extended period. 
The memory cell is controlled by three gates: the input gate, the forget gate, and the output gate.

The input gate controls how much new information should be stored in the memory cell. The forget gate determines which information from the memory cell 
should be destroyed or forgotten. The output gate controls how much information is output from the memory cell to the next time step. 
These gates are controlled by activation functions, which are commonly sigmoid and tanh functions, 
and allow the LSTM to selectively update, forget, and output data from the memory cell.

(38)

The Gated Recurrent Unit (GRU) model is a type of recurrent neural network (RNN) architecture that has been widely used in natural language processing (NLP) tasks. 
It is designed to address the vanishing gradient problem and capture long-term dependencies in sequential data.

GRU is similar to LSTM in that it incorporates gating mechanisms, but it has a simplified architecture with fewer gates, 
making it computationally more efficient and easier to train. The GRU model consists of the following components:

Hidden State: 

The hidden state in GRU represents the learned representation or memory of the input sequence up to the current time step. 
It retains and passes information from the past to the present.

Update Gate: The update gate in GRU controls the flow of information from the past hidden state to the current time step. 
It determines how much of the previous information should be retained and how much new information should be incorporated.

Reset Gate: The reset gate in GRU determines how much of the past information should be discarded or forgotten. 
It helps in removing irrelevant information from the previous hidden state.

Candidate Activation: The candidate activation represents the new information to be added to the hidden state. 
It is computed based on the current input and a transformed version of the previous hidden state using the reset gate

(39)

Sequence-to-sequence (Seq2Seq) is a type of neural network that is used for natural language processing (NLP) tasks.
It is a type of recurrent neural network (RNN) that can learn long-term word relationships.
This makes it ideal for tasks like machine translation, text summarization, and question answering.

Here’s how the Seq2Seq model works:

Encoder:
 
The encoder transforms the input sequence, such as a sentence in the source language, into a fixed-length vector representation known as the “context vector” 
or “thought vector”. To capture sequential information from the input, the encoder commonly employs recurrent neural networks (RNNs) such as 
Long Short-Term Memory (LSTM) or Gated Recurrent Units (GRU).

Context Vector: 

The encoder’s context vector acts as a summary or representation of the input sequence. It encodes the meaning and important information from the 
input sequence into a fixed-size vector, regardless of the length of the input.

Decoder: 

The decoder uses the encoder’s context vector to build the output sequence, which could be a translation or a summarised version. 
It is another RNN-based network that creates the output sequence one token at a time. At each step, the decoder can be conditioned on the context vector, which serves as an initial hidden state.

Backpropagation through time (BPTT) is a technique commonly used to train Seq2Seq models. The model is optimized to minimize the difference between the 
predicted output sequence and the actual target sequence.

(40)

An attention mechanism is a kind of neural network that uses an additional attention layer within an Encoder-Decoder neural network that enables the model to 
focus on specific parts of the input while performing a task. 
It achieves this by dynamically assigning weights to different elements in the input, 
indicating their relative importance or relevance. This selective attention allows the model to focus on relevant information, capture dependencies, and 
analyze relationships within the data.

Transformer is one of the fundamental models in NLP based on the attention mechanism, which allows it to capture long-range dependencies in sequences 
more effectively than traditional recurrent neural networks (RNNs). 
It has given state-of-the-art results in various NLP tasks like word embedding, machine translation, text summarization, question answering etc.

Some of the key advantages of using a Transformer are as follows:

Parallelization: 

The self-attention mechanism allows the model to process words in parallel, which makes it significantly faster to train compared to sequential models like RNNs.

Long-Range Dependencies: 

The attention mechanism enables the Transformer to effectively capture long-range dependencies in sequences, which makes it suitable for tasks where 
long-term context is essential.

State-of-the-Art Performance: 

Transformer-based models have achieved state-of-the-art performance in various NLP tasks, such as machine translation, 
language modelling, text generation, and sentiment analysis.

The key components of the Transformer model are as follows:

Self-Attention Mechanism
Encoder-Decoder Network
Multi-head Attention
Positional Encoding
Feed-Forward Neural Networks
Layer Normalization and Residual Connections

(41)

The self-attention mechanism is a powerful tool that allows the Transformer model to capture long-range dependencies in sequences. 
It allows each word in the input sequence to attend to all other words in the same sequence, and the model learns to assign weights to each word based on 
its relevance to the others. This enables the model to capture both short-term and long-term dependencies, which is critical for many NLP applications.

The purpose of the multi-head attention mechanism in Transformers is to allow the model to recognize different types of correlations and patterns in the input sequence. 
In both the encoder and decoder, the Transformer model uses multiple attention heads.

Each attention head learns to pay attention to different parts of the input, allowing the model to capture a wide range of characteristics and dependencies.

Positional encoding is applied to the input embeddings to offer this positional information like the relative or absolute position of each word in the 
sequence to the model. These encodings are typically learnt and can take several forms, including sine and cosine functions or learned embeddings. 
This enables the model to learn the order of the words in the sequence, which is critical for many NLP tasks.



(42)

The Transformer architecture can be described in depth below:

A. Encoder:

The encoder takes an input sequence of tokens (e.g., words) as input and transforms each token into a vector representation known as an embedding. 
Positional encoding is used in these embeddings to preserve the order of the words in the sequence.

An encoder consists of multiple self-attention layers and each self-attention layer is used to capture relationships and dependencies between words in the sequence.

After the self-attention step, the output representations of the self-attention layer are fed into a feed-forward neural network. 
This network applies the non-linear transformations to each word’s contextualised representation independently.

Residual connections and layer normalisation are used to back up the self-attention and feed-forward layers. The residual connections in deep networks 
help to mitigate the vanishing gradient problem, and layer normalisation stabilises the training process.

B. Decoder:

Similar to the encoder, the decoder takes an input sequence and transforms each token into embeddings with positional encoding.

Unlike the encoder, the decoder uses masked self-attention in the self-attention layers. This masking ensures that the decoder can only attend to places 
before the current word during training, preventing the model from seeing future tokens during generation.

Cross-attention layers in the decoder allow it to attend to the encoder’s output, which enables the model to use information from the input sequence 
during output sequence generation.

Similar to the encoder, the decoder’s self-attention output passes through feed-forward neural networks.

The decoder also includes residual connections and layer normalization to help in training and improve model stability.

C. Final Output Layer:

The final output layer is a softmax layer that transforms the decoder’s representations into probability distributions over the vocabulary. 
This enables the model to predict the most likely token for each position in the output sequence.

(43)

Generative models are trained to generate new data that is similar to the data that was used to train them.  
For example, a generative model could be trained on a dataset of text and code and then used to generate new text or code that is similar to the text and code 
in the dataset. Generative models are often used for tasks such as text generation, machine translation, and creative writing.

Discriminative models are trained to recognise different types of data. A discriminative model. For example, a discriminative model could be trained on a dataset 
of labelled text and then used to classify new text as either spam or ham. Discriminative models are often used for tasks such as text classification, 
sentiment analysis, and question answering.

(44)

BLEU stands for “Bilingual Evaluation Understudy”. It is a metric invented by IBM in 2001 for evaluating the quality of a machine translation. 
It measures the similarity between machine-generated translations with the professional human translation.

The BLEU score is measured by comparing the n-grams (sequences of n words) in the machine-translated text to the n-grams in the reference text. 
The higher BLEU Score signifies, that the machine-translated text is more similar to the reference text.

The BLEU (Bilingual Evaluation Understudy) score is calculated using n-gram precision and a brevity penalty.

The n-gram precision is the ratio of matching n-grams in the machine-generated translation to the total number of n-grams in the reference translation.

Brevity Penalty measures the length difference between machine-generated translations and reference translations. While finding the BLEU score, 
It penalizes the machine-generated translations if that is found too short compared to the reference translation’s length with exponential decay.

brevity-penalty=min(1,exp(1− Machine translation length/Reference length))


BLEU score is calculated by taking the geometric mean of the individual n-gram precisions and then adjusting it with the brevity penalty.

BLEU score= brevity penalty * PI_i^n (Precision_i)^(1/n)

The BLEU score goes from 0 to 1, with higher values indicating better translation quality and 1 signifying a perfect match to the reference translation.

(45)


Natural Language Processing(NLP) Tasks  --->  Evaluation Metric



Part-of-Speech Tagging (POS Tagging) or Named Entity Recognition (NER)  -->  Accuracy, F1-score, Precision, Recall

Dependency Parsing  -->  UAS (Unlabeled Attachment Score), LAS (Labeled Attachment Score)

Coreference resolution -->	B-CUBED, MUC, CEAF

Text Classification or Sentiment Analysis -->	Accuracy, F1-score, Precision, Recall

Machine Translation  -->	BLEU (Bilingual Evaluation Understudy), METEOR (Metric for Evaluation of Translation with Explicit Ordering)

Text Summarization  -->	ROUGE (Recall-Oriented Understudy for Gisting Evaluation), BLEU

Question Answering -->	F1-score, Precision, Recall, MRR(Mean Reciprocal Rank)

Text Generation  -->	Human evaluation (subjective assessment), perplexity (for language models)

Information Retrieval  -->	Precision, Recall, F1-score, Mean Average Precision (MAP)

Natural language inference (NLI)  -->	Accuracy, precision, recall, F1-score, Matthews correlation coefficient (MCC)

Topic Modeling  -->	Coherence Score, Perplexity

Speech Recognition  -->	Word Error Rate (WER)

Speech Synthesis (Text-to-Speech) -->	Mean Opinion Score (MOS)


Accuracy: Accuracy is the percentage of predictions that are correct.

Precision: Precision is the percentage of correct predictions out of all the predictions that were made.

Recall: Recall is the percentage of correct predictions out of all the positive cases.

F1-score: F1-score is the harmonic mean of precision and recall.

MAP(Mean Average Precision): MAP computes the average precision for each query and then averages those precisions over all queries.

MUC(Mention-based Understudy for Coreference): MUC is a metric for coreference resolution that measures the number of mentions that are correctly identified and linked.

B-CUBED: B-cubed is a metric for coreference resolution that measures the number of mentions that are correctly identified, linked, and ordered.

CEAF: CEAF is a metric for coreference resolution that measures the similarity between the predicted coreference chains and the gold standard coreference chains.

ROC AUC: ROC AUC is a metric for binary classification that measures the area under the receiver operating characteristic curve.

MRR: MRR is a metric for question answering that measures the mean reciprocal rank of the top-k-ranked documents.

Perplexity:

Perplexity is a language model evaluation metric. It assesses how well a linguistic model predicts a sample or test set of previously unseen data. 
Lower perplexity values suggest that the language model is more predictive.

BLEU: BLEU is a metric for machine translation that measures the n-gram overlap between the predicted translation and the gold standard translation.

METEOR: METEOR is a metric for machine translation that measures the overlap between the predicted translation and the gold standard translation, 
        taking into account synonyms and stemming.

WER(Word Error Rate): WER is a metric for machine translation that measures the word error rate of the predicted translation.

MCC: MCC is a metric for natural language inference that measures the Matthews correlation coefficient between the predicted labels and the gold standard labels.

ROUGE: ROUGE is a metric for text summarization that measures the overlap between the predicted summary and the gold standard summary, 
       taking into account n-grams and synonyms.

Human Evaluation (Subjective Assessment): Human experts or crowd-sourced workers are asked to submit their comments, evaluations, or rankings on 
                                          many elements of the NLP task’s performance in this technique.

----------------------------------------------------------------------------------------------------------------------------------------------------------

(46)

A few of the libraries of the NLTK package that we often use in NLP are:

SequentialBackoffTagger
DefaultTagger
UnigramTagger
treebank
wordnet
FreqDist
patterns
RegexpTagger
backoff_tagger
UnigramTagger, BigramTagger, and TrigramTagger

(47)

The techniques used for syntactic analysis are as follows:


A. Parsing: It helps in deciding the structure of a sentence or text in a document. It helps analyze the words in the text based on the grammar of the language.
B. Word segmentation: The segmentation of words segregates the text into small significant units.
C. Morphological segmentation: The purpose of morphological segmentation is to break words into their base form.
D. Stemming: It is the process of removing the suffix from a word to obtain its root word.
E. Lemmatization: It helps combine words using suffixes, without altering the meaning of the word

(48)

Techniques used for semantic analysis are as given below:

A. Named entity recognition: This is the process of information retrieval that helps identify entities such as 
the name of a person, organization, place, time, emotion, etc.

B. Word sense disambiguation: It helps identify the sense of a word used in different sentences.

C. Natural language generation: It is a process used by the software to convert structured data into human-spoken languages. 
By using NLG, organizations can automate content for custom reports.

(49)

Latent semantic indexing is a mathematical technique used to improve the accuracy of the information retrieval process.
The design of LSI algorithms allows machines to detect the hidden (latent) correlation between semantics (words).

The technique used for information understanding is called singular value decomposition. It is generally used to handle static and unstructured data. 
The matrix obtained for singular value decomposition contains rows for words and columns for documents. 
This method is best suited to identify components and group them according to their types.

The main principle behind LSI is that words carry a similar meaning when used in a similar context. Computational LSI models are slow in comparison to other models. 

(50)

Google uses TF-IDF to decide the index of search results according to the relevancy of pages. The design of the TF-IDF algorithm helps optimize the search results in 
Google. It helps quality content rank up in search results.

(51)

Dependency parsing helps assign a syntactic structure to a sentence. Therefore, it is also called syntactic parsing. 
Dependency parsing is one of the critical tasks in NLP. It allows the analysis of a sentence using parsing algorithms. 
Also, by using the parse tree in dependency parsing, we can check the grammar and analyze the semantic structure of a sentence.

For implementing dependency parsing, we use the spaCy package. It implements token properties to operate the dependency parse tree.

(52)

Pragmatic analysis is an important task in NLP for interpreting knowledge that is lying outside a given document.

Example: Consider this sentence: ‘Do you know what time it is?’

This sentence can either be asked for knowing the time or for yelling at someone to make them note the time. This depends on the context in which we use the sentence.

Pragmatic ambiguity refers to the multiple descriptions of a word or a sentence. An ambiguity arises when the meaning of the sentence is not clear.

(53)

