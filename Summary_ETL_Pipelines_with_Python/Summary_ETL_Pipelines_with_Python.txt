################################################## Part 1:Introduction to ETL, Data Pipelines, and Design Principles #########################################


-- A Primer on Python and the Development Environment

(1) We will walk you through a step-by-step tutorial on how to
install and establish a basic Git-tracked development environment that will prevent future confounding modular
incompatibilities from impacting the successful deployment of your data pipelines in production.

(2) We can also write functions inside a class. A function inside a class is known as a method.

(3) The first step of any programming project is to instantiate a version control repository unique to your environment. This
keeps the project’s development and production environments in separate buckets.

We will be using GitHub to track and store our data pipelines throughout this book.

(4) It’s best practice to get into the habit of always committing and pushing your changes frequently so that your work is
backed up with Git; this way, you won’t lose all of your hard-earned code if you ever do something silly such as spill
coffee all over your laptop.

(5) Git version control enables collaboration without the fear of
losing or overwriting changes since multiple developers can work on different branches, and changes can be reviewed
and merged through pull requests.

(6) pip freeze >> requirements.txt

(7) The concept of circular dependency is not always talked about when first learning Python, but it’s a concept where one
or more modules depend on each other. Ex: Class A depends on Class B and Class B depends on Class A.

(8) To create an
internally consistent environment, versions of the dependencies must be flexibly adjusted to account for the circular
interdependencies of imports. This magic of MMS begins!

(9) Utilizing module management systems (MMSs):

MMSs are like special folders that only work in certain environments. They do this by changing sys.prefix and
sys.exec_prefix so that they point to the base directory of the virtual environment. This is helpful because it lets
developers create “clean” applications and also makes sure that all the different parts of the project work well together.

There are many different module management systems to choose from, but Anaconda is the most popular. However, it
doesn’t always have the most up-to-date packages for data engineers, and pip, the regular package manager for Python,
doesn’t work well with Anaconda. That’s why we’re using pipenv in this book. It’s a virtual environment and package
management system that uses Pipfile and Pipfile.lock, similar to a requirements.txt file.

(10) Configuring a Pipenv environment in PyCharm:

In Python development, managing project environments is crucial to keep your project’s dependencies organized and
controlled. One way to achieve this is by using pipenv. Let’s start the process by installing Pipenv. Open your Terminal
and execute the following command:

(base) usr@project % pip install --user pipenv

This command instructs pip (a Python package manager) to install Pipenv in your user space. The --user option
ensures that Pipenv is installed in the user install directory for your platform.

Once installed, remember to activate the pipenv environment before you begin to work on your new project. This way,
the entirety of your project is developed within the isolated virtual environment.

(11) activate virtual environment:

pipenv shell

(12) Installing Packages:

pip- packages can be added or removed from the environment via simple $ pipenv install or $ pipenv uninstall
commands since activating the pipenv environment is designed to replace the need for the pip- tag in the command
line.

(13) Pipfile and Pipfile.lock:

When a pipenv environment is initiated, an empty Pipfile is automatically created. As mentioned previously, Pipfile
is synonymous with the requirements.txt file.

Pipfile.lock is created to specify which version of the dependencies referenced in Pipfile should be used to avoid
automatic upgrades of packages that depend on each other. You can run the $ pipenv lock command to update the
Pipfile.lock file with the currently used versions of all the dependencies within your virtual environment. However,
pipenv takes care of updating the Pipfile and Pipfile.lock files with each package installation.

Example:

pipenv install numba

(check output)

(14)

A data pipeline is a series of tasks, such as transformations, filters, aggregations, and merging multiple sources, before
outputting the processed data into some target. In layman’s terms, a data pipeline gets data from the “source” to the
“target,”.

Ex. A mail can be filtered as spam and non-spam and send to the target.

(15)

packets of raw data are ingested into the entry of the pipeline and, through a series of steps and processes, 
the raw material is formatted and packaged into an output location, which is most commonly used for storage.

(16)

The following are the attributes of a robust data pipeline:

• Clearly defined expectations
• Scalable architecture
• Reproducible and clear

(17) A robust data pipeline should have clearly defined expectations in terms of the data it is processing and the results it is
expected to produce. This includes specifying the types and sources of data, as well as the desired output format and any
required transformations or aggregations. Having clearly defined expectations helps ensure that the pipeline is
consistently producing accurate and reliable results.

(18)

A robust data pipeline should have a scalable architecture that can handle
increasing volumes of data without degradation in performance. This may involve using distributed systems or
implementing efficient algorithms to process the data promptly.

(19)

Reproducibility and clarity are also important attributes of a robust data pipeline. The pipeline should be able to produce
the same results each time it is run, and the steps involved in processing the data should be documented and easy to
understand. This helps ensure that the pipeline can easily be maintained and modified as needed.

(20)

it is important to have a clear understanding of both the input and output data structures when designing a
data pipeline. This includes knowing the data structures, any potential issues with the data (such as corruption), and the
frequency of new data creation for the input data. For the output data, it is important to understand the structural
requirements to ensure that the pipeline consistently produces the desired result.


(21)

Before writing a line of code, take some time to sketch out a flowchart of the transformations that need to be performed
on the input data to produce the output data. Go a little deeper into how Python packages can be used to accomplish
these transformations. This is how you create the foundation for operational stability in your code, where you apply
Don’t Repeat Yourself (DRY) practices to create non-redundant code that is thoroughly tested and easy to logically
follow.

DRY practices are a programming principle that aims to reduce redundancy in code. This means avoiding writing the
same code multiple times, and instead using functions, variables, and other code structures to reuse code whenever
possible.


(22)

Creating project diagrams is fundamental to a project that is not only clear, reproducible, and fully
strategized, but also makes it highly shareable and transparent.

(23) ETL data pipeline:

ETL stands for Extract, Transform, and Load. In an ELT process, data is first extracted from a source, then
transformed and formatted in a specific way, and finally loaded into a final storage location.

(24)

ELT stands for Extract, Load, and Transform, and is similar to ETL, but the data is first loaded into the target system
and then transformed within the target system.

(25)

Here are a few factors that you might consider when deciding between ETL and ELT:

• Data volume: If the volume of data is very large, ELT might be more efficient because the transformation step can
be done in parallel within the target system.

• Data transformation requirements: If the data needs to undergo complex transformations, it might be easier to
perform the transformations in the target system using ELT

• Source system capabilities: If the source system is not able to perform the necessary transformations, ETL might be
the only option

• Target system capabilities: If the target system is not able to efficiently handle the load phase of the ETL process,
ELT might be a better option

• Data latency: If real-time data movement is required, ELT might be a better choice because it allows the data to be
loaded and transformed more quickly

(26)

In general, ETL is more commonly used when the source and target systems are different and the data needs to be
transformed in a specific way before it is loaded into the target system. ELT is more commonly used when the target
system is more powerful and can handle the transformation step itself.

(27)

There are three main types of ETL pipelines:

• Batch processing

• Streaming

• Cloud-native

(28) Batch processing:

Batch processing is a method of data processing that involves dividing a large volume of data into smaller pieces, or
batches, and processing each batch separately. This is often done when a project requires a large amount of data to be
handled (mostly in TB or above), but the downstream use of the data only requires it to be available asynchronously.
Batch processing is a popular choice for handling large amounts of data because it allows engineers to design a process
that can run one batch of data through the pipeline at a time, rather than trying to process all of the data at once.

One example of a batch processing system is a company that processes customer orders for an online store. The company
receives a large volume of orders each day and needs to process this data to update inventory levels, generate invoices,
and fulfill orders. To do this, they use a batch processing system to break the data into smaller chunks and run each
chunk through their data pipeline. This allows them to process the data efficiently, without overwhelming their systems
or causing delays in order processing

(29) Streaming method:

Real-time data solutions are necessary when a project needs to immediately process fresh data. Streaming methods are
often used in these situations as they allow data to flow continuously, which may be variable and subject to sudden
changes in structure. To handle these challenges, a combination of data pipelines is typically utilized. There are several
advanced tools, such as Apache Storm and Apache Samza, that can efficiently process real-time data.

An example of this is an e-commerce website that needs to process real-time user data as shopping is in progress. The
use of real-time data processing, combined with AI/ML, can result in an enhanced user shopping experience.

(30) Cloud-native:

Most of the famous public cloud platforms, such as Amazon Web Services (AWS), Google Cloud Platform (GCP), and 
Microsoft Azure, provide inbuilt data processing capabilities that come with various integrated tools and technologies 
that can be used to build a robust and resilient ETL pipeline.

(31) Automating ETL pipelines:

To streamline and optimize the ETL process in a production environment, there are several tools and technologies
available to automate the pipeline.

There are several key benefits to automating ETL pipelines:

• Data democratization: Automating the ETL process can make it easier for a wider range of users to access and use
data since the process of extracting, transforming, and loading data is streamlined and made more efficient

• Robust data availability and access: By automating the ETL process, data is made more consistently available and
accessible to users since the pipelines are designed to run regularly and can be easily configured to handle any
changes or updates to the source data

• Team focus: Automating ETL pipelines can free up team members to focus on more important tasks such as data
analytics, developing machine learning models, and other higher-value activities, rather than spending time on
manual data preparation tasks

• Onboarding process: Automated ETL pipelines can make it easier and faster to onboard new team members since
the process of extracting and preparing data is already streamlined and automated, reducing the need for new hires to
learn complex manual processes

• Schema management: Automating ETL pipelines can also help with schema management since the pipelines can be
configured to handle changes to the source data schema automatically, ensuring that the data is always properly
transformed and loaded into the target system

(32) By utilizing tools such as AWS Lambda and EC2 alongside Step Functions, it is simple to orchestrate or automate ETL
pipelines on a cloud platform. For instance, we can implement these resources to automate our pipelines effectively.

Orchestrating ETL pipelines can also be done using open source tools such as Apache Airflow, Luigi, and others (more
on this in Parts 2 and 3).

(33) Benefits of ETL pipelines:

• Allow developers and engineers to focus on useful tasks rather than worrying about data

• Free up time for developers, engineers, and scientists to focus on actual work

• Help organizations move data from one place to another and transform it into a desired format efficiently
and systematically

(34) Applications of ETL pipelines:

• Migrating data from a legacy platform to the cloud and vice versa

• Centralizing data sources to have a consolidated view of data

• Providing stable data sources for data-driven applications and data analytic tools

• Acting as a blueprint for organizational data, serving as a single source of truth

(35) Example of an ETL pipeline in action:

• Netflix has a very robust ETL pipeline that manages petabytes of data, allowing them to employ a small
team of engineers to handle admin tasks related to data

(36) Overall benefits of ETL pipelines:

• Saves money in the long run

• Helps with business expansion

-----------------------------------------------------------------------------------------------------------------------

(37) Chapter 3: Design Principles for Creating Scalable and Resilient Pipelines

The true art of data engineering is the architecture of the pipeline design. 

(38) Even though the term was coined in 1958, business intelligence (BI) is a rather new discipline with a lot of research
activity; its popularity in information systems has only gained popularity in the last few years.

(39)  The Extract, Transform, and Load (ETL) perspective for data pipelines has helped shaped the complexity of BI activities into neat building blocks to
identify and represent frequently recurring patterns within an organization’s scope of data movement needs.

(40) you can easily separate each “Import-Process-Export” step into a different process. But
what happens when a network issue occurs mid-operation? What happens to the data in the middle of the pipeline? Is any
data retained? It quickly becomes clear that things get a little hairy for basic pipeline designs

redundant processing power is required to redo any data transactions lost in a network failure.
Thus, there is a pressing need for intermittent data storage.

(41) To address some of the data loss and processing power challenges found in the most basic ETL design, engineers added a
Persistent Staging Area (PSA) layer to the pipeline to preserve segments of data source conversions during the data
migration, even if the source system is facing network or latency issues.

(42) the PSA layer breaks up the data importation and data exportation stages to limit the liability of data
movement issues from the connection between the source and sink data locations being severed. In practice, staging
locations have a wide variety of applications and use cases, so we like to think of this as a “sanity check” step. For
instance, some organizations store a master staging file to measure the data integrity of fresh data imports with the
existing data expectations of the system. However, what happens if the connection to the data source is inconsistent, or
on a different frequency than what’s needed for the business needs for data ingestion? The PSA layer alone can’t
accommodate this need; it looks like there’s a need for an additional layer.

(43) With an added volatile staging area (VSA) layer, the ETL-VP is the glow-up version of the ETL-P pipeline design
pattern.

The ETL-VP can handle asynchronous data importation processes with ease due to its ability to use the VSA layer to
batch-load data into the PSA layer on a predictable and consistent schedule. This additional layer also further minimizes
the impact of network connectivity issues by maintaining the ETL’s performance at a predictable frequency. However, as
engineers, there’s always one nagging curiosity: “It works, but can we make this better?” Yes, we can.

(44) ELT two-phase pattern:

The full scope two-phase ETL design pattern is the crème de la crème. While the ELT-VP compensates for many
connectivity issues, it is a high computational investment to sync the data loading process with the data collection
process. In other words, it can become a money-munging machine.

As it turns out, breaking the collection of sources and the loading of data into separate steps is a handy way to reduce
some of these preventable costs. Building a data pipeline in this fashion gives more control over the pipeline as well as
makes each process more dynamic and reusable, as the same method for data importation can be used with customizable
and interchangeable methods of loading the data with various other filters and audit mechanisms.

(45)

It’s important to keep in mind that most Python modules rely on the CPUs available on your local device, which means
that one problem with Pandas is its processing capacity. When data is imported with Pandas, the data is stored in the
local memory on your device during the duration of the script. This becomes problematic, and very quickly, as multiple
data copies are created of larger and larger datasets, even if only during a script’s cycle.

(46)

 Both Pandas and NumPy require a contiguous allocation of memory, thus, simple data manipulation operations become quite
costly as each new version of the data is stored in contiguous memory locations. Even in large-capacity environments,
both packages perform rather poorly (and unbearably slow) on large datasets. Since the premise of this book is creating
reliable, scalable, data pipelines, restricting our code base to Pandas and NumPy simply won’t do.

(47) Scaling for big data packages:

Dask:

When faced with processing capacity contingencies, it makes sense to increase your capacity with the assistance of
additional devices. This is the premise for the parallelization of tasks in Python. Similar to batch processing, partitioning
data (aka separating the data into equal, bite-sized bits), allows large data sources to be processed in identical,
synchronous ways across multiple systems.


Dask is the Python library that allows for the parallelization of processing tasks in a flexible and dynamic scope. Dask is
a cloud-based module that uses “clusters”, which are extra processing units in the cloud on standby, that can lend a
helping hand to heavy-processing tasks initiated on your local device. The creators of Dask designed this parallelization
module to be an extension of popular data processing modules, such as Pandas and NumPy, so Dask’s syntax mimics the
same format.

(48)

#dask dataframe mimics pandas
import dask.dataframe as dd
df=dd.read_csv("x.csv")
df.groupby(df.userid).value.mean().compute()

# dask array mimimcs numpy
import dask.array as da
f=h5py.file("myfile.hdf5")
x=da.from_array(['/big_data'],chunks=(1000,1000))
x=x.mean(axis=1).compute()

(49)

The Dask library is an incredibly powerful and ubiquitous tool, but it can be equally as complex to get properly
acquainted with. While we will intermittently review the core uses of Dask throughout the remainder of this book, we
encourage you to take some time and go through the tutorials available within Dask’s documentation to explore more
complex applications: https://pypi.org/project/dask/

(50)

Numba:

Similar to Dask, Numba is another powerhouse module specifically designed to enhance numerical processes in Python.
One of the main complaints about data processing in Python is its overall speed, even without dealing with big data files.
This is partially due to Python’s “preference” to wait to compile data before a command is executed. Numba uses a justin-time (JIT) compilation (also dynamic translation), which brings Python processing speeds to be more in line with its
humble C/C++ beginnings. With Numba, there’s no need to run a separate compilation step; all you need to do is add
Numba decorators before a Python function, and Numba does the rest:


(51)

from numba import njit
import random

@njit
def monte_carlo_pi(nsamples):
	acc=0
	for i in range(nsamples):
		x=random.random()
		y=random.random()
		if (x**2 + y**2) < 1.0:
			acc +=1
	return 4.0*acc/nsamples
(52)

Numba works well with basic Python libraries such as Pandas and NumPy, distributed execution frameworks such as
Dask, and Python development environment tools such as Jupyter notebooks, so we will dive more deeply into the use
cases of Numba throughout the remainder of the book. Please install Numba into your virtual environment with the
following code:
(Project) usr@project %% pipenv install numba
Feel free to check out more in the Numba documentation: https://pypi.org/project/numba/.


-------------------------------------------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------------------------------------------

(53) Part 2: Part 2:Designing ETL Pipelines with Python


